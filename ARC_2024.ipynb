{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuc9aVHMh3ioR0jYurMD7A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RRG314/topological-adam/blob/paper/ARC_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YasGaYM7egm",
        "outputId": "d56ef540-d600-41f6-8664-1e57b5ec642e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¥ Downloading ARC dataset...\n",
            "âœ… Extracted ARC data.\n",
            "âœ… Found 400 training tasks.\n",
            "âœ… Using device: cpu\n",
            "\n",
            "ðŸ”¹ Task 1/20: 007bbfb7.json\n",
            "Adam                 | Ep  200 | Loss=6.711e-03\n",
            "Adam                 | Ep  400 | Loss=1.035e-03\n",
            "Adam                 | Ep  600 | Loss=4.743e-04\n",
            "Adam                 | Ep  800 | Loss=2.425e-04\n",
            "Adam                 | Ep 1000 | Loss=1.185e-04\n",
            "Adam+RDT             | Ep  200 | Loss=3.121e-03\n",
            "Adam+RDT             | Ep  400 | Loss=8.749e-04\n",
            "Adam+RDT             | Ep  600 | Loss=3.699e-04\n",
            "Adam+RDT             | Ep  800 | Loss=1.796e-04\n",
            "Adam+RDT             | Ep 1000 | Loss=9.696e-05\n",
            "TopologicalAdam      | Ep  200 | Loss=9.885e-04\n",
            "TopologicalAdam      | Ep  400 | Loss=2.318e-04\n",
            "TopologicalAdam      | Ep  600 | Loss=6.046e-05\n",
            "TopologicalAdam      | Ep  800 | Loss=1.890e-05\n",
            "TopologicalAdam      | Ep 1000 | Loss=7.369e-06\n",
            "TopologicalAdam+RDT  | Ep  200 | Loss=8.291e-04\n",
            "TopologicalAdam+RDT  | Ep  400 | Loss=2.781e-04\n",
            "TopologicalAdam+RDT  | Ep  600 | Loss=1.399e-04\n",
            "TopologicalAdam+RDT  | Ep  800 | Loss=7.655e-05\n",
            "TopologicalAdam+RDT  | Ep 1000 | Loss=4.461e-05\n",
            "\n",
            "ðŸ”¹ Task 2/20: 00d62c1b.json\n",
            "Adam                 | Ep  200 | Loss=1.129e-09\n",
            "Adam                 | Ep  400 | Loss=0.000e+00\n",
            "Adam                 | Ep  600 | Loss=0.000e+00\n",
            "Adam                 | Ep  800 | Loss=0.000e+00\n",
            "Adam                 | Ep 1000 | Loss=0.000e+00\n",
            "Adam+RDT             | Ep  200 | Loss=1.322e-05\n",
            "Adam+RDT             | Ep  400 | Loss=1.125e-05\n",
            "Adam+RDT             | Ep  600 | Loss=9.649e-06\n",
            "Adam+RDT             | Ep  800 | Loss=7.992e-06\n",
            "Adam+RDT             | Ep 1000 | Loss=6.409e-06\n",
            "TopologicalAdam      | Ep  200 | Loss=2.326e-10\n",
            "TopologicalAdam      | Ep  400 | Loss=0.000e+00\n",
            "TopologicalAdam      | Ep  600 | Loss=0.000e+00\n",
            "TopologicalAdam      | Ep  800 | Loss=0.000e+00\n",
            "TopologicalAdam      | Ep 1000 | Loss=8.047e-16\n",
            "TopologicalAdam+RDT  | Ep  200 | Loss=1.938e-05\n",
            "TopologicalAdam+RDT  | Ep  400 | Loss=1.411e-05\n",
            "TopologicalAdam+RDT  | Ep  600 | Loss=1.009e-05\n",
            "TopologicalAdam+RDT  | Ep  800 | Loss=7.089e-06\n",
            "TopologicalAdam+RDT  | Ep 1000 | Loss=4.899e-06\n",
            "\n",
            "ðŸ”¹ Task 3/20: 017c7c7b.json\n",
            "Adam                 | Ep  200 | Loss=8.275e-10\n",
            "Adam                 | Ep  400 | Loss=0.000e+00\n",
            "Adam                 | Ep  600 | Loss=0.000e+00\n",
            "Adam                 | Ep  800 | Loss=0.000e+00\n",
            "Adam                 | Ep 1000 | Loss=0.000e+00\n",
            "Adam+RDT             | Ep  200 | Loss=5.803e-04\n",
            "Adam+RDT             | Ep  400 | Loss=5.932e-05\n",
            "Adam+RDT             | Ep  600 | Loss=2.618e-05\n",
            "Adam+RDT             | Ep  800 | Loss=2.059e-05\n",
            "Adam+RDT             | Ep 1000 | Loss=1.600e-05\n",
            "TopologicalAdam      | Ep  200 | Loss=1.707e-10\n",
            "TopologicalAdam      | Ep  400 | Loss=0.000e+00\n",
            "TopologicalAdam      | Ep  600 | Loss=0.000e+00\n",
            "TopologicalAdam      | Ep  800 | Loss=0.000e+00\n",
            "TopologicalAdam      | Ep 1000 | Loss=0.000e+00\n",
            "TopologicalAdam+RDT  | Ep  200 | Loss=9.927e-05\n",
            "TopologicalAdam+RDT  | Ep  400 | Loss=1.067e-05\n",
            "TopologicalAdam+RDT  | Ep  600 | Loss=3.167e-06\n",
            "TopologicalAdam+RDT  | Ep  800 | Loss=2.529e-06\n",
            "TopologicalAdam+RDT  | Ep 1000 | Loss=2.290e-06\n",
            "\n",
            "ðŸ”¹ Task 4/20: 025d127b.json\n",
            "Adam                 | Ep  200 | Loss=7.067e-03\n",
            "Adam                 | Ep  400 | Loss=1.788e-03\n",
            "Adam                 | Ep  600 | Loss=4.877e-04\n",
            "Adam                 | Ep  800 | Loss=1.291e-04\n",
            "Adam                 | Ep 1000 | Loss=3.177e-05\n",
            "Adam+RDT             | Ep  200 | Loss=3.924e-03\n",
            "Adam+RDT             | Ep  400 | Loss=7.171e-04\n",
            "Adam+RDT             | Ep  600 | Loss=2.525e-04\n",
            "Adam+RDT             | Ep  800 | Loss=1.368e-04\n",
            "Adam+RDT             | Ep 1000 | Loss=9.222e-05\n",
            "TopologicalAdam      | Ep  200 | Loss=5.696e-04\n",
            "TopologicalAdam      | Ep  400 | Loss=5.915e-05\n",
            "TopologicalAdam      | Ep  600 | Loss=2.098e-05\n",
            "TopologicalAdam      | Ep  800 | Loss=1.134e-05\n",
            "TopologicalAdam      | Ep 1000 | Loss=6.537e-06\n",
            "TopologicalAdam+RDT  | Ep  200 | Loss=1.442e-03\n",
            "TopologicalAdam+RDT  | Ep  400 | Loss=4.315e-04\n",
            "TopologicalAdam+RDT  | Ep  600 | Loss=2.109e-04\n",
            "TopologicalAdam+RDT  | Ep  800 | Loss=1.371e-04\n",
            "TopologicalAdam+RDT  | Ep 1000 | Loss=9.431e-05\n",
            "\n",
            "ðŸ”¹ Task 5/20: 045e512c.json\n",
            "Adam                 | Ep  200 | Loss=1.163e-02\n",
            "Adam                 | Ep  400 | Loss=5.040e-04\n",
            "Adam                 | Ep  600 | Loss=1.693e-04\n",
            "Adam                 | Ep  800 | Loss=9.311e-05\n",
            "Adam                 | Ep 1000 | Loss=5.120e-05\n",
            "Adam+RDT             | Ep  200 | Loss=9.260e-03\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# ARC-AGI 2024 BENCHMARK â€” RDT + Topological Adam (Pro)\n",
        "# FINAL FIXED VERSION â€” safe RDT projection\n",
        "# ============================================================\n",
        "\n",
        "import os, json, time, math, zipfile, requests, csv\n",
        "import torch, torch.nn as nn, torch.autograd as autograd\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================================================\n",
        "# === Embedded RDT Kernel ====================================\n",
        "# ============================================================\n",
        "\n",
        "def rdt_kernel(L, alpha=0.5, D=0.1, dx=1.0):\n",
        "    \"\"\"Simple recursive diffusion transform kernel.\"\"\"\n",
        "    L = torch.nan_to_num(L, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    up, down = torch.roll(L, 1, 0), torch.roll(L, -1, 0)\n",
        "    left, right = torch.roll(L, 1, 1), torch.roll(L, -1, 1)\n",
        "    diff = (up + down + left + right - 4 * L)\n",
        "    step = L + alpha * D * diff / (dx * dx)\n",
        "    return torch.tanh(step)\n",
        "\n",
        "def safe_rdt_apply(v):\n",
        "    \"\"\"Applies RDT kernel safely on a 1D tensor and returns SAME-LENGTH vector.\"\"\"\n",
        "    n = v.numel()\n",
        "    s = int(math.sqrt(n))\n",
        "    if s * s < n:\n",
        "        s += 1\n",
        "    pad_len = s * s - n\n",
        "    padded = F.pad(v, (0, pad_len), value=0.0)\n",
        "    grid = padded.view(s, s)\n",
        "    rdt_grid = rdt_kernel(grid)\n",
        "    flat = rdt_grid.flatten()[:n]\n",
        "    # normalize to match original range\n",
        "    flat = (flat - flat.mean()) / (flat.std() + 1e-8)\n",
        "    return flat.view_as(v)\n",
        "\n",
        "# ============================================================\n",
        "# === Embedded Topological Adam ==============================\n",
        "# ============================================================\n",
        "\n",
        "class TopologicalAdam(torch.optim.Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9,0.999), eps=1e-8,\n",
        "                 eta=0.1, mu0=1.0, w_topo=0.05, E_target=1.0):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        eta=eta, mu0=mu0, w_topo=w_topo, E_target=E_target)\n",
        "        super().__init__(params, defaults)\n",
        "        self.alpha = {}; self.beta = {}\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure: loss = closure()\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                grad = p.grad.data\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    state[\"step\"] = 0\n",
        "                    state[\"exp_avg\"] = torch.zeros_like(p)\n",
        "                    state[\"exp_avg_sq\"] = torch.zeros_like(p)\n",
        "                    self.alpha[p] = torch.zeros_like(p)\n",
        "                    self.beta[p] = torch.zeros_like(p)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
        "                beta1, beta2 = group[\"betas\"]\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
        "                denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
        "                step_size = group[\"lr\"]\n",
        "\n",
        "                J = (self.alpha[p] - self.beta[p]) * grad\n",
        "                eta, mu0 = group[\"eta\"], group[\"mu0\"]\n",
        "                self.alpha[p] = (1 - eta) * self.alpha[p] + (eta / mu0) * J\n",
        "                self.beta[p] = (1 - eta) * self.beta[p] - (eta / mu0) * J\n",
        "\n",
        "                E = 0.5 * (self.alpha[p].pow(2).mean() + self.beta[p].pow(2).mean())\n",
        "                if E > group[\"E_target\"]:\n",
        "                    scale = math.sqrt(group[\"E_target\"] / (E + 1e-12))\n",
        "                    self.alpha[p].mul_(scale)\n",
        "                    self.beta[p].mul_(scale)\n",
        "\n",
        "                p.addcdiv_(exp_avg, denom, value=-step_size)\n",
        "                p.add_(torch.tanh(self.alpha[p] - self.beta[p]) * -group[\"w_topo\"])\n",
        "                state[\"step\"] += 1\n",
        "        return loss\n",
        "\n",
        "# ============================================================\n",
        "# === ARC Data Loader ========================================\n",
        "# ============================================================\n",
        "\n",
        "def get_arc_data():\n",
        "    url = \"https://github.com/fchollet/ARC-AGI/archive/refs/heads/master.zip\"\n",
        "    fname = \"arc_data.zip\"\n",
        "    if not os.path.exists(\"arc_data\"):\n",
        "        print(\"ðŸ“¥ Downloading ARC dataset...\")\n",
        "        r = requests.get(url)\n",
        "        open(fname, \"wb\").write(r.content)\n",
        "        with zipfile.ZipFile(fname, \"r\") as z:\n",
        "            z.extractall(\"arc_data\")\n",
        "        print(\"âœ… Extracted ARC data.\")\n",
        "    base = \"arc_data/ARC-AGI-master/data/training\"\n",
        "    files = sorted([os.path.join(base, f) for f in os.listdir(base) if f.endswith(\".json\")])\n",
        "    print(f\"âœ… Found {len(files)} training tasks.\")\n",
        "    return files[:20]\n",
        "\n",
        "# ============================================================\n",
        "# === Simple MLP =============================================\n",
        "# ============================================================\n",
        "\n",
        "class TinyMLP(nn.Module):\n",
        "    def __init__(self, input_dim=2, hidden=64, output_dim=1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden), nn.Tanh(),\n",
        "            nn.Linear(hidden, hidden), nn.Tanh(),\n",
        "            nn.Linear(hidden, output_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ============================================================\n",
        "# === Benchmark ==============================================\n",
        "# ============================================================\n",
        "\n",
        "def run_arc_benchmark():\n",
        "    tasks = get_arc_data()\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"âœ… Using device: {device}\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for idx, file in enumerate(tasks):\n",
        "        with open(file, \"r\") as f:\n",
        "            task = json.load(f)\n",
        "        print(f\"\\nðŸ”¹ Task {idx+1}/{len(tasks)}: {os.path.basename(file)}\")\n",
        "\n",
        "        in_grids = [torch.tensor(t[\"input\"], dtype=torch.float32) for t in task[\"train\"]]\n",
        "        out_grids = [torch.tensor(t[\"output\"], dtype=torch.float32) for t in task[\"train\"]]\n",
        "\n",
        "        # --- Safe padding for variable sizes ---\n",
        "        max_h = max(max(g.shape[0] for g in in_grids), max(g.shape[0] for g in out_grids))\n",
        "        max_w = max(max(g.shape[1] for g in in_grids), max(g.shape[1] for g in out_grids))\n",
        "\n",
        "        def pad_to_size(g, h, w):\n",
        "            pad_h = h - g.shape[0]\n",
        "            pad_w = w - g.shape[1]\n",
        "            return F.pad(g, (0, pad_w, 0, pad_h), value=0.0)\n",
        "\n",
        "        in_grids = [pad_to_size(g, max_h, max_w) for g in in_grids]\n",
        "        out_grids = [pad_to_size(g, max_h, max_w) for g in out_grids]\n",
        "\n",
        "        x = torch.cat([g.flatten().unsqueeze(1) for g in in_grids], dim=0)\n",
        "        y = torch.cat([g.flatten().unsqueeze(1) for g in out_grids], dim=0)\n",
        "        data = torch.cat([x, y], dim=1)\n",
        "        data = (data - data.mean()) / (data.std() + 1e-8)\n",
        "\n",
        "        def train_model(opt_name, opt_class, use_rdt=False, epochs=1000):\n",
        "            model = TinyMLP(2 if not use_rdt else 3).to(device)\n",
        "            opt = opt_class(model.parameters(), lr=1e-3)\n",
        "            loss_fn = nn.MSELoss()\n",
        "            start = time.time()\n",
        "\n",
        "            for ep in range(1, epochs + 1):\n",
        "                opt.zero_grad()\n",
        "                X = data[:, :1].to(device)\n",
        "                Y = data[:, 1:].to(device)\n",
        "                inp = torch.cat([X, Y], dim=1)\n",
        "                if use_rdt:\n",
        "                    rdt_feat = safe_rdt_apply(inp.mean(dim=1))  # compress across dims safely\n",
        "                    inp = torch.cat([inp, rdt_feat.unsqueeze(1)], dim=1)\n",
        "                pred = model(inp)\n",
        "                loss = loss_fn(pred, Y)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "                if ep % 200 == 0:\n",
        "                    print(f\"{opt_name:<20} | Ep {ep:4d} | Loss={loss.item():.3e}\")\n",
        "            return time.time() - start, loss.item()\n",
        "\n",
        "        combos = [\n",
        "            (\"Adam\", torch.optim.Adam, False),\n",
        "            (\"Adam+RDT\", torch.optim.Adam, True),\n",
        "            (\"TopologicalAdam\", TopologicalAdam, False),\n",
        "            (\"TopologicalAdam+RDT\", TopologicalAdam, True),\n",
        "        ]\n",
        "\n",
        "        for name, opt, rdt_flag in combos:\n",
        "            t, L = train_model(name, opt, rdt_flag)\n",
        "            results.append(dict(Task=file, Optimizer=name, FinalLoss=L, Time=t))\n",
        "\n",
        "    # --- Save results ---\n",
        "    with open(\"arc_results.csv\", \"w\", newline=\"\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=results[0].keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerows(results)\n",
        "    print(\"âœ… Results saved: arc_results.csv\")\n",
        "\n",
        "    # --- Plot summary ---\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for opt in [\"Adam\", \"Adam+RDT\", \"TopologicalAdam\", \"TopologicalAdam+RDT\"]:\n",
        "        vals = [r[\"FinalLoss\"] for r in results if r[\"Optimizer\"] == opt]\n",
        "        plt.plot(vals, label=opt)\n",
        "    plt.yscale(\"log\")\n",
        "    plt.legend()\n",
        "    plt.title(\"ARC 2024 Benchmark â€“ Loss per Task\")\n",
        "    plt.xlabel(\"Task\")\n",
        "    plt.ylabel(\"Final Loss (log scale)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"arc_benchmark.png\")\n",
        "    print(\"âœ… Saved: arc_benchmark.png\")\n",
        "\n",
        "    print(\"\\nâœ… All ARC-AGI benchmarks completed.\")\n",
        "    return results\n",
        "\n",
        "# ============================================================\n",
        "# === Run ====================================================\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_arc_benchmark()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"arc_results.csv\")\n",
        "summary = df.groupby(\"Optimizer\")[\"FinalLoss\"].agg([\"mean\",\"median\",\"min\",\"max\"])\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "eF1k95xv7l3F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}